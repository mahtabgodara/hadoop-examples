sudo apt-get install python-software-properties
sudo apt-get install openjdk-6-jdk
sudo apt-get install openssh-server
wget http://archive.apache.org/dist/hadoop/core/hadoop-1.0.4/hadoop-1.0.4.tar.gz

vi /home/ubuntu/.bashrc

export JAVA_HOME=/usr/lib/jvm/java-6-openjdk-i386
export HADOOP_HOME=/home/hduser/hadoop-1.0.4
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin

close and open a new terminal and verify:

java -version

uncomment the following lines in $HADOOP_HOME/conf/hadoop-env.sh

export JAVA_HOME=/usr/lib/jvm/java-6-openjdk-i386
export HADOOP_HEAPSIZE=1000

share the ssh keys of root user, share the key with every slave node 1 by 1

sudo su -
ssh-keygen -t rsa -P ""
cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys

verify

ssh root@localhost

the above command should allow you to login without password, try this for each slave as well.

mkdir -p /app/hadoop/tmp

add the following to core-site.xml in the configurations section

<property>
  <name>hadoop.tmp.dir</name>
  <value>/app/hadoop/tmp</value>
  <description>A base for other temporary directories.</description>
</property>

<property>
  <name>fs.default.name</name>
  <value>hdfs://localhost:54310</value>
  <description>The name of the default file system.  A URI whose
  scheme and authority determine the FileSystem implementation.  The
  uri's scheme determines the config property (fs.SCHEME.impl) naming
  the FileSystem implementation class.  The uri's authority is used to
  determine the host, port, etc. for a filesystem.</description>
</property>


add the following to mapred-site.xml in the configurations section

<property>
  <name>mapred.job.tracker</name>
  <value>localhost:54311</value>
  <description>The host and port that the MapReduce job tracker runs
  at.  If "local", then jobs are run in-process as a single map
  and reduce task.
  </description>
</property>


add the following to hdfs-site.xml in the configurations section

<property>
  <name>dfs.replication</name>
  <value>1</value>
  <description>Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  </description>
</property>

hadoop namenode -format

bin/start-all.sh

ps -ef | grep java

jps

wget http://archive.apache.org/dist/hadoop/core/hadoop-1.0.4/hadoop-1.0.4.tar.gz
wget https://archive.apache.org/dist/hive/hive-0.12.0/hive-0.12.0-bin.tar.gz
wget http://archive.apache.org/dist/flume/1.4.0/apache-flume-1.4.0-bin.tar.gz
wget http://apache.tradebit.com/pub/pig/pig-0.12.0/pig-0.12.0.tar.gz
wget http://www.motorlogy.com/apache/hbase/stable/hbase-0.98.3-hadoop1-bin.tar.gz
wget mirrors.koehn.com/apache/spark/spark-1.0.0/spark-1.0.0-bin-hadoop1.tgz
wget http://apache.spinellicreations.com/hadoop/common/hadoop-2.4.0/hadoop-2.4.0.tar.gz

tar -xzvf hive-0.12.0-bin.tar.gz
tar -xzvf apache-flume-1.4.0-bin.tar.gz
tar -xzvf pig-0.12.0.tar.gz
tar -xzvf hbase-0.98.3-hadoop1-bin.tar.gz
tar -xzvf spark-1.0.0-bin-hadoop1.tgz
tar -xzvf hadoop-2.4.0.tar.gz


sudo apt-get install mysql-server

sudo netstat -tap | grep mysql

tcp        0      0 localhost:mysql         *:*                LISTEN      2556/mysqld

sudo service mysqld restart

mysql -u root -p

default port for mysql is 3306

sudo apt-get install curl

wget http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.31.zip
curl -L 'http://www.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.22.tar.gz/from/http://mysql.he.net/' | tar xz
sudo cp mysql-connector-java-5.1.22/mysql-connector-java-5.1.22-bin.jar /root/hive-0.12.0-bin/lib/

sudo /usr/bin/mysql_secure_installation

NYNNY

mysql -u root -p

CREATE DATABASE metastore;
USE metastore;
SOURCE /root/hive-0.12.0-bin/scripts/metastore/upgrade/mysql/hive-schema-0.12.0.mysql.sql;

CREATE USER 'hive'@'localhost' IDENTIFIED BY 'mypassword';
REVOKE ALL PRIVILEGES, GRANT OPTION FROM 'hive'@'localhost';
GRANT SELECT,INSERT,UPDATE,DELETE,LOCK TABLES,EXECUTE ON metastore.* TO 'hive'@'localhost';
FLUSH PRIVILEGES;
quit;

create the file conf/hive-site.xml and add the following to it:

<?xml version="1.0"?>
<configuration>
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://localhost:3306/metastore</value>
  <description>the URL of the MySQL database</description>
</property>

<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>mypassword</value>
</property>

<property>
  <name>datanucleus.autoCreateSchema</name>
  <value>false</value>
</property>

<property>
  <name>datanucleus.fixedDatastore</name>
  <value>true</value>
</property>

<property>
  <name>datanucleus.autoStartMechanism</name> 
  <value>SchemaTable</value>
</property> 
</configuration>


$HADOOP_HOME/bin/hadoop fs -mkdir       /tmp
$HADOOP_HOME/bin/hadoop fs -mkdir       /user/hive/warehouse
$HADOOP_HOME/bin/hadoop fs -chmod g+w   /tmp
$HADOOP_HOME/bin/hadoop fs -chmod g+w   /user/hive/warehouse

add the following to ~/.bashrc

export HIVE_HOME=/root/hive-0.12.0-bin
export PATH=$PATH:$HIVE_HOME\bin

source ~/.bashrc

To launch hive shell 

hive

show databases;
show tables;

use metastore;

you can now create tables and use Hive QL.


PIG

tar xvzf pig-*.gz


export PIG_INSTALL=/root/pig-0.12.0
export PIG_CLASSPATH=$HADOOP_HOME/conf
export PATH=$PATH:$HIVE_HOME/bin:$PIG_INSTALL/bin

export HADOOP_HOME=<path_to_hadoop_install>

export HCAT_HOME=<path_to_hcat_install>

export HIVE_HOME=<path_to_hive_install>

export PIG_CLASSPATH=$HCAT_HOME/share/hcatalog/hcatalog-*.jar:\
$HIVE_HOME/lib/hive-metastore-*.jar:$HIVE_HOME/lib/libthrift-*.jar:\
$HIVE_HOME/lib/hive-exec-*.jar:$HIVE_HOME/lib/libfb303-*.jar:\
$HIVE_HOME/lib/jdo2-api-*-ec.jar:$HIVE_HOME/conf:$HADOOP_HOME/conf:\
$HADOOP_HOME/lib/slf4j-api-*.jar

export PIG_OPTS=-Dhive.metastore.uris=thrift://<hostname>:<port>


export PIG_CLASSPATH=$HCAT_HOME/share/hcatalog/hive-hcatalog-*.jar:$HIVE_HOME/lib/hive-metastore-*.jar:$HIVE_HOME/lib/libthrift-*.jar:$HIVE_HOME/lib/hive-exec-*.jar:$HIVE_HOME/lib/libfb303-*.jar:$HIVE_HOME/lib/jdo-api-*.jar:$HIVE_HOME/conf:$HADOOP_HOME/conf:$HIVE_HOME/lib/slf4j-api-*.jar


HBASE

tar -xvf hbase-0.94.8.tar.gz

vi conf/hbase-env.sh

umcomment the following environment variable and set their values right.

export JAVA_HOME=/usr/lib/jvm/java-6-openjdk-i386
export HBASE_REGIONSERVERS=${HBASE_HOME}/conf/regionservers
export HBASE_MANAGES_ZK=true

vi ~/.bashrc

export HBASE_HOME=/root/hbase-0.98.3-hadoop1
export PATH=$PATH:$HIVE_HOME/bin:$PIG_INSTALL/bin:$HBASE_HOME/bin


Add the following properties to conf/hbase-site.xml

<property>

<name>hbase.rootdir</name>

<value>hdfs://localhost:54310/hbase</value>

</property>

<property>

<name>hbase.cluster.distributed</name>

<value>true</value>

</property>

<property>

<name>hbase.zookeeper.quorum</name>

<value>localhost</value>

</property>

<property>

<name>dfs.replication</name>

<value>1</value>

</property>

<property>

<name>hbase.zookeeper.property.clientPort</name>

<value>2181</value>

</property>

<property>

<name>hbase.zookeeper.property.dataDir</name>

<value>/root/hbase/zookeeper</value>

</property>


start-hbase.sh

jps

hbase shell

create 'test','info'

if you see a warning you would need to do the following:

hbase classpath | tr ":" "\n" | grep -i slf4j 

remove the following 2 files from hbase/lib

/root/hbase-0.98.3-hadoop1/lib/slf4j-api-1.6.4.jar
/root/hbase-0.98.3-hadoop1/lib/slf4j-log4j12-1.6.4.jar

relaunch 'hbase shell'

try to use hbase CLI commands and this time, there should be no warnings


OOZIE
https://oozie.apache.org/docs/3.3.0/DG_QuickStart.html
https://oozie.apache.org/docs/3.3.0/DG_Examples.html



add the following entry to $HADOOP_HOME/conf/core-site.xml

  <!-- OOZIE -->
  <property>
    <name>hadoop.proxyuser.[OOZIE_SERVER_USER].hosts</name>
    <value>[OOZIE_SERVER_HOSTNAME]</value>
  </property>
  <property>
    <name>hadoop.proxyuser.[OOZIE_SERVER_USER].groups</name>
    <value>[USER_GROUPS_THAT_ALLOW_IMPERSONATION]</value>
  </property>

Copy the oozie directory from <location provided>

cd to the oozie directory

bin/oozie-setup.sh

bin/ooziedb.sh create -sqlfile oozie.sql -run

bin/oozie-start.sh

bin/oozie admin -oozie http://localhost:11000/oozie -status

hadoop fs -put examples examples

Run an example

oozie job -oozie http://localhost:11000/oozie -config examples/apps/map-reduce/job.properties -run

oozie job -oozie http://localhost:11000/oozie -info <job-id>


Oozie with mysql

$ mysql -u root -p
Enter password: ******

mysql> create database oozie;
Query OK, 1 row affected (0.03 sec)

mysql>  grant all privileges on oozie.* to 'oozie'@'localhost' identified by 'oozie';
Query OK, 0 rows affected (0.03 sec)

mysql>  grant all privileges on oozie.* to 'oozie'@'%' identified by 'oozie';
Query OK, 0 rows affected (0.03 sec)

mysql> exit
Bye

 Edit properties in the oozie-site.xml file as follows:

...
    <property>
        <name>oozie.service.JPAService.jdbc.driver</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    <property>
        <name>oozie.service.JPAService.jdbc.url</name>
        <value>jdbc:mysql://localhost:3306/oozie</value>
    </property>
    <property>
        <name>oozie.service.JPAService.jdbc.username</name>
        <value>oozie</value>
    </property>
    <property>
        <name>oozie.service.JPAService.jdbc.password</name>
        <value>oozie</value>
    </property>
    ...
    
Add the MySQL JDBC driver JAR to Oozie.







FLUME


agent.sources=s1
agent.channels=c1
agent.sinks=k1
agent.sources.s1.type=exec
agent.sources.s1.command = tail -F /root/sample.log
agent.sources.s1.channels=c1
agent.sources.s1.bind=0.0.0.0
agent.sources.s1.port=12345
agent.channels.c1.type=memory
agent.sinks.k1.type=hdfs
agent.sinks.k1.channel=c1
agent.sinks.k1.hdfs.path = hdfs://localhost:54310/tmp/samplelogs/


bin/flume-ng agent --conf ./conf/ -f conf/hdfs.conf -Dflume.root.logger=DEBUG,console -n agent

import time
f = open('/root/sample.log', 'w')
for i in xrange(1,1000000):
  f.write(str(i) + ": a \n")
  time.sleep(1)
  f.flush()

f.close()






wget https://archive.apache.org/dist/hive/hive-0.12.0/hive-0.12.0-bin.tar.gz
wget http://archive.apache.org/dist/flume/1.4.0/apache-flume-1.4.0-bin.tar.gz
wget http://apache.tradebit.com/pub/pig/pig-0.12.0/pig-0.12.0.tar.gz
wget http://www.motorlogy.com/apache/hbase/stable/hbase-0.98.3-hadoop1-bin.tar.gz
wget mirrors.koehn.com/apache/spark/spark-1.0.0/spark-1.0.0-bin-hadoop1.tgz
wget http://apache.spinellicreations.com/hadoop/common/hadoop-2.4.0/hadoop-2.4.0.tar.gz
wget http://downloads.typesafe.com/scala/2.11.1/scala-2.11.1.tgz
wget http://apache.mesi.com.ar/sqoop/1.4.4/sqoop-1.4.4.bin__hadoop-1.0.0.tar.gz
wget http://archive.apache.org/dist/hadoop/core/hadoop-1.0.4/hadoop-1.0.4.tar.gz
wget https://www.dropbox.com/s/odl88f4ctcoz30v/oozie-3.3.0.tar.gz

sudo apt-get install mysql-server
sudo apt-get install curl

curl -L 'http://www.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.22.tar.gz/from/http://mysql.he.net/'

sudo apt-get install python-software-properties
sudo apt-get install openjdk-6-jdk
sudo apt-get install openssh-server # if not already installed
sudo apt-get install maven





/Users/mahtab.singh/Downloads/ubuntu-12.10-desktop-i386/ubuntu-12.10-desktop-i386.vdi





